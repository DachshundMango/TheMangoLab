{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.]) tensor([1.])\n",
      "tensor([0.3673], requires_grad=True) tensor([0.1355], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(1, requires_grad=True) # 추적을 시작할 텐서를 생성\n",
    "y= torch.rand(1)\n",
    "y.requires_grad = True # 추적을 시작할 텐서를 생성\n",
    "loss = y - x\n",
    "\n",
    "loss.backward() # 연산에 연결된 각 텐서들의 미분값을 계산하여, 각 텐서의 grad 속성에 저장\n",
    "print(x.grad, y.grad) # x.grad = -1, y.grad = 1 출력\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7801, 0.0470, 0.4449],\n",
      "        [0.9690, 0.9784, 0.4082],\n",
      "        [0.6538, 0.9327, 0.5938],\n",
      "        [0.6383, 0.8683, 0.6383]], requires_grad=True) tensor([0.1741, 0.4542, 0.1515], requires_grad=True) tensor([3.2152, 3.2807, 2.2367], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.ones(3)\n",
    "W = torch.rand(4, 3, requires_grad=True)\n",
    "b = torch.rand(3, requires_grad=True)\n",
    "z = torch.matmul(x, W) + b\n",
    "print(W, b, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8795, grad_fn=<MseLossBackward0>) tensor([[1.4768, 1.5205, 0.8245],\n",
      "        [1.4768, 1.5205, 0.8245],\n",
      "        [1.4768, 1.5205, 0.8245],\n",
      "        [1.4768, 1.5205, 0.8245]]) tensor([1.4768, 1.5205, 0.8245])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss = F.mse_loss(z, y) # z 는 모델의 예측값, y는 실제값\n",
    "loss.backward()\n",
    "print(loss, W.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(3.8795, grad_fn=<MseLossBackward0>) tensor([3.2152, 3.2807, 2.2367], grad_fn=<AddBackward0>) tensor([1., 1., 1.])\n",
      "2 tensor(1.7242, grad_fn=<MseLossBackward0>) tensor([2.4768, 2.5205, 1.8245], grad_fn=<AddBackward0>) tensor([1., 1., 1.])\n",
      "3 tensor(0.7663, grad_fn=<MseLossBackward0>) tensor([1.9845, 2.0137, 1.5496], grad_fn=<AddBackward0>) tensor([1., 1., 1.])\n",
      "4 tensor(0.3406, grad_fn=<MseLossBackward0>) tensor([1.6564, 1.6758, 1.3664], grad_fn=<AddBackward0>) tensor([1., 1., 1.])\n",
      "5 tensor(0.1514, grad_fn=<MseLossBackward0>) tensor([1.4376, 1.4505, 1.2443], grad_fn=<AddBackward0>) tensor([1., 1., 1.])\n",
      "6 tensor(0.0673, grad_fn=<MseLossBackward0>) tensor([1.2917, 1.3003, 1.1629], grad_fn=<AddBackward0>) tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.1\n",
    "learning_rate = 0.1\n",
    "iteration_num = 0\n",
    "\n",
    "while loss > threshold:\n",
    "    iteration_num += 1\n",
    "    W = W - learning_rate * W.grad\n",
    "    b = b - learning_rate * b.grad\n",
    "    print(iteration_num, loss, z, y)\n",
    "\n",
    "    W.detach_().requires_grad_(True) # 가중치 업데이트를 한번 끊고, 다시 텐서 추적을 시작\n",
    "    b.detach_().requires_grad_(True) # 편향 업데이트를 한번 끊고, 다시 텐서 추적을 시작 \n",
    "\n",
    "    z = torch.matmul(x, W) + b # 업데이트된 가중치와 편향으로 다시 계산\n",
    "    loss = F.mse_loss(z, y) # 새로운 손실값 계산\n",
    "    loss.backward() # 새로운 손실값에 대한 미분값 계산, 다시 가중치와 편향의 grad 속성에 저장\n",
    "\n",
    "print(iteration_num + 1, loss, z, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
